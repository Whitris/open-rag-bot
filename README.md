# Open RAG Bot

![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)
![MIT License](https://img.shields.io/badge/license-MIT-green.svg)

A modular chatbot framework powered by Retrieval-Augmented Generation (RAG).
Ask questions about your documents and get context-aware, LLM-generated answers.

## Features

- **Retrieval-Augmented Generation (RAG):** Answers are grounded in your own documents, not just LLM training data.
- **Modular design:** Switch between different LLM and embedding providers (OpenAI, Groq, etc.) with a single config change.
- **Flexible document ingestion:** Easily add your PDFs, DOCX, or text files to the knowledge base.
- **Fast search:** Uses [ChromaDB](https://www.trychroma.com/) for efficient vector search and retrieval with metadata support.
- **Two interfaces:** Use from the command line (CLI) or via a modern Streamlit webapp.
- **Tested and extensible:** Includes unit tests and a clean codebase for easy customization.

## Folder structure

```markdown
open-rag-chatbot/
│
├── src/
│ ├── app/ # Streamlit webapp
│ ├── core/ # Core logic: retrieval, prompt, chatbot
│ ├── services/ # LLM & embedding clients (OpenAI, Groq, etc)
│ ├── data/ # Data processing scripts and utilities
│ └── config/ # Settings, .env, config files
│
├── tests/ # Unit and integration tests
├── data/ # Local, non-versioned data (indexed files, outputs)
├── cli.py
├── process_data.py
├── generate_embeddings.py
├── requirements.txt
├── pyproject.toml
├── README.md
├── .gitignore
└── .pre-commit-config.yaml
```

## Quickstart

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/open-rag-chatbot.git
cd open-rag-chatbot
```

### 2. Install dependencies (recommended: PDM)

```bash
pip install pdm
pdm install
```

If you prefer classic pip:

```bash
pip install -r requirements.txt
```

### 3. Configure environment
Copy .env.example to .env and add your API keys (see instructions in the next section).

### 4. Prepare your documents
Place your PDF, DOCX or text files in the folder specified by input_dir (see src/config/settings.py or .env).

### 5. Process your documents

Extracts text from documents, splits it into chunks, and saves the result as a CSV file.

```bash
pdm run python process_data.py [INPUT_DIR] [OUTPUT_CSV] [OPTIONS]
```

#### Required Arguments

- INPUT_DIR: Directory containing the documents to process (PDF, TXT, DOCX, etc.).
- OUTPUT_CSV: Path to save the output CSV file.

#### Options

- --chunk-size: Size of each text chunk, in characters (default: 500).
- --max-files: Maximum number of files to process (default: -1, meaning all files).
- --formats: File formats to include, comma-separated (default: pdf,txt,docx).

#### Example usage

```bash
pdm run python process_data.py ./documents ./output/chunks.csv --chunk-size 1000 --max-files 100 --formats pdf,txt
```

### 6. Generate embeddings

Creates a vector index of the text chunks using embeddings and stores the collection in ChromaDB.

```bash
pdm run python generate_embeddings.py [CSV_PATH] [OPTIONS]
```

#### Required Arguments

- CSV_PATH: Path to the CSV file containing text chunks generated by the previous step.

#### Options

- --chroma-dir: Directory to store the ChromaDB index (default: as defined in your settings).
- --collection-name: Name of the ChromaDB collection (default: default_collection or as defined in your settings).

#### Example usage

```bash
pdm run python generate_embeddings.py ./output/chunks.csv --chroma-dir ./output/chroma --collection-name asp_collection
```

### 7. Start the chatbot

Command-line interface (CLI)
```bash
pdm run python cli.py
```

Webapp
```bash
pdm run streamlit run src/app/main.py
```

## Environment variables

All sensitive settings (such as API keys and provider selection) are managed via environment variables. Most defaults can be set or overridden in `src/config/.env`.

1. Copy the example file:

    ```bash
    cp src/config/.env.example src/config/.env
    ```

2. Edit src/config/.env to add your credentials and adjust preferences as needed.

### Main variables  
| Variable             | Description                                 | Example      |
|----------------------|---------------------------------------------|--------------|
| OPENAI_API_KEY       | Your OpenAI API key                         | sk-...       |
| GROQ_API_KEY         | Your Groq API key                           | gsk_...      |
| EMBEDDING_PROVIDER   | Embedding provider to use                   | openai/groq  |
| LLM_PROVIDER         | LLM provider to use                         | openai/groq  |
| LIGHT_LLM_MODEL      | Model name for fast/re-writing tasks        | gpt-4.1-nano |
| LLM_MODEL            | Model name for main answer generation       | gpt-4.1      |
| ICON_PATH            | (Optional) Path to the logo/icon for the UI | src/assets/logo.png  | 

#### Note:

- If a provider (Groq or OpenAI) is selected, the corresponding API key must be set. The application will fail if the required key is missing.
- If you do not specify a model, the code will select a default based on the chosen provider.

#### Advanced configuration and file paths

Other settings, such as file paths, directories, and collection names, can be modified in src/config/settings.py, or overridden with environment variables if needed.

Default directories and files include:

- Input documents: `input_docs/docs/`
- Processed CSV: `data/processed/stuff.csv`
- ChromaDB index: `data/index/chromadb/`
- ChromaDB collection name: `documents`


## Test and development

This project includes unit and integration tests to ensure correctness and maintainability.

### Running tests

To run all tests with [pytest](https://pytest.org/):

```bash
pdm run pytest
```

or, if using classic pip:

```bash
pytest
```

### Pre-commit hooks

This project uses [pre-commit](https://pre-commit.com/) to automate code formatting and linting via [Ruff](https://docs.astral.sh/ruff/).

To enable automatic checks before every commit, install pre-commit and set up the hooks:

```bash
pip install pre-commit
pre-commit install
```

After setup, every commit will automatically run ruff and ruff-format on your codebase.

- Configuration is in `.pre-commit-config.yaml`
- You can manually run all hooks on all files with:

```bash
pre-commit run --all-files
```

## Contributing Guidelines

We welcome contributions, suggestions, and pull requests!
Please follow these guidelines to keep the codebase clean, maintainable, and friendly for everyone.

### How to contribute

1. Fork the repository and create a new branch for your feature or fix.
2. Write clear, concise code following project conventions (see Code Style below).
3. Add tests for new features or bug fixes, when appropriate.
4. Run all tests before submitting.
5. Run code style checks and apply formatting.
6. Use pre-commit hooks (recommended).
7. Open a pull request with a clear title and description. Link related issues if relevant.

### Code style

- This project uses [Ruff](https://docs.astral.sh/ruff/) to enforce Python code style and linting.
- Please follow [PEP8](https://peps.python.org/pep-0008/) guidelines where possible.
- Organize imports and use descriptive names for functions, classes, and variables.

To check code style and run linting on the src/ and tests/ directories:

```bash
pdm run ruff check src/ tests/
```

Or, if using pip:

```bash
ruff check src/ tests/
```

For auto-formatting, you can also run:

```bash
pdm run ruff format src/ tests/
```

## License

This project is licensed under the MIT License.

## Contact

For questions, suggestions, or professional inquiries, contact:

- GitHub issues: [GitHub Issues](https://github.com/whitris/open-rag-chatbot/issues)
- Email: <nicola.marcantognini@outlook.com>
- LinkedIn: [My LinkedIn Profile](https://www.linkedin.com/in/nicola-marcantognini/)

Feel free to open an issue or reach out directly!

## Credits

Maintained by [Whitris](https://github.com/Whitris).

This project makes use of:
- [Streamlit](https://streamlit.io/) for the web interface
- [ChromaDB](https://www.trychroma.com/) for vector search and storage
- [Typer](https://typer.tiangolo.com/) for the CLI
- [OpenAI](https://platform.openai.com/) and [Groq](https://console.groq.com/) APIs for LLM and embeddings
